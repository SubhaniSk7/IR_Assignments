{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os,glob\n",
    "from sklearn.externals import joblib\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contract(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def tokenize(text):\n",
    "    words=word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "def regTokenize(text):\n",
    "    tok=RegexpTokenizer('[A-Za-z0-9]?\\w+')\n",
    "    return tok.tokenize(text)\n",
    "\n",
    "def removePunct(text):\n",
    "    words=wordpunct_tokenize(text)\n",
    "    return words\n",
    "\n",
    "def removePunctuations(words):\n",
    "#     https://www.programiz.com/python-programming/examples/remove-punctuation\n",
    "    punctuations='''!()-.\"[]\".{};:'\"\\,<>./---::--...''``?@#$%^&*_~'''\n",
    "    removed=[]\n",
    "    for char in words:\n",
    "        if char not in punctuations:\n",
    "            removed.append(char)\n",
    "        else:\n",
    "            continue\n",
    "    return removed\n",
    "    \n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemma(words):\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=WordNetLemmatizer().lemmatize(words[i])\n",
    "    return words\n",
    "\n",
    "def stemming(words):\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=porter_stemmer.stem(words[i])\n",
    "    return words\n",
    "\n",
    "def removestopwords(text):\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in text:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def remove_numbers(words):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirdSet\n",
      "thirdSet\\comp.graphics\n",
      "============================================================\n",
      "thirdSet\\rec.motorcycles\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "terms=[]\n",
    "document_ids=[]\n",
    "\n",
    "path = 'thirdSet'\n",
    "print(path)\n",
    "for dire in glob.glob(os.path.join(path, '*')):\n",
    "    print(dire)\n",
    "    for filename in glob.glob(os.path.join(dire, '*')):\n",
    "        st=filename.split('\\\\')\n",
    "#         print(st[2])\n",
    "        s=st[1]+'/'\n",
    "        s=s+st[2]\n",
    "        \n",
    "#         document_ids.append(st[2])\n",
    "        document_ids.append(s)\n",
    "\n",
    "        f=open(filename,'r+')\n",
    "        data=f.read()\n",
    "\n",
    "        data=lowercase(data)\n",
    "        data=contract(data)\n",
    "        data=regTokenize(data)\n",
    "        data=lemma(data)\n",
    "        data=stemming(data)\n",
    "\n",
    "        data=list(data)\n",
    "\n",
    "        for i in range(0,len(data)):\n",
    "            t=data[i]\n",
    "#             id=st[2]\n",
    "            id=s\n",
    "            row=[]\n",
    "            \n",
    "            row.append(t)\n",
    "            row.append(id)\n",
    "            row.append(i+1)\n",
    "            terms.append(row)\n",
    "    print('============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "633528\n"
     ]
    }
   ],
   "source": [
    "print(len(document_ids))\n",
    "print(len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_list = sorted(terms, key=itemgetter(0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in sorted_list:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary={}\n",
    "for i in sorted_list:\n",
    "    ind=i[0]\n",
    "    docid=i[1]\n",
    "    if(ind not in dictionary.keys()):\n",
    "        dictionary[ind]=[]\n",
    "        docfreq=1\n",
    "        dictionary[ind].append(docfreq)\n",
    "        \n",
    "        doc_dict={}\n",
    "        doc_dict[docid]=[i[2]]\n",
    "        dictionary[ind].append(doc_dict)\n",
    "    else:\n",
    "        if(docid not in dictionary.get(ind)[1].keys()):\n",
    "            dictionary.get(ind)[1][docid]=[i[2]]\n",
    "        else:\n",
    "            dictionary.get(ind)[1].get(docid).append(i[2])\n",
    "            \n",
    "        dictionary.get(ind)[0]=len(dictionary.get(ind)[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document_ids_positional_index.sav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentIds_filename='document_ids_positional_index.sav'\n",
    "joblib.dump(document_ids,documentIds_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terms_positional_index.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_filename='terms_positional_index.sav'\n",
    "joblib.dump(terms, terms_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sorted_list_positional_index.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list_filename='sorted_list_positional_index.sav'\n",
    "joblib.dump(sorted_list,sorted_list_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dictionary_positional_index.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_filename='dictionary_positional_index.sav'\n",
    "joblib.dump(dictionary,dictionary_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26474"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = joblib.load('dictionary_positional_index.sav')\n",
    "docids=joblib.load('document_ids_positional_index.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(docids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_test(term):\n",
    "    term=lowercase(term)\n",
    "    term=contract(term)\n",
    "    term=regTokenize(term)\n",
    "    term=lemma(term)\n",
    "    term=stemming(term)\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    print(query)\n",
    "    distinct_words=set(query)\n",
    "    distinct_words=list(distinct_words)\n",
    "#     ---------------------------------------------\n",
    "    q=[]#getting keys from positional index for each token in query\n",
    "    for i in range(0,len(distinct_words)):\n",
    "        q.append(di.get(distinct_words[i])[1].keys())\n",
    "    \n",
    "    z=0\n",
    "    for i in q:\n",
    "        print(distinct_words[z],'-->',len(i))\n",
    "        z+=1\n",
    "#     ---------------------------------------------\n",
    "    keys_set=[] # converting list to set to perform intersection\n",
    "    for i in range(0,len(q)):\n",
    "        keys_set.append(set(q[i]))\n",
    "    \n",
    "#     for i in keys_set:\n",
    "#         print(i)\n",
    "#         print('\\n')\n",
    "#     ---------------------------------------------\n",
    "    commom_docs_set=set.intersection(*keys_set)  # getting common_docs set => intersection of list of sets\n",
    "#     print('\\ncommon docs set:-->',commom_docs_set)\n",
    "    \n",
    "    common_docs_list=list(commom_docs_set)  # converting common docs set to list and sort\n",
    "    common_docs_list.sort()\n",
    "#     print('\\ncommon docs list:-->',common_docs_list)\n",
    "#     ---------------------------------------------\n",
    "    searched_docs=[]\n",
    "    for i in range(0,len(common_docs_list)):\n",
    "        doc_list_position=[]\n",
    "        doc_id=common_docs_list[i]\n",
    "        for j in range(0,len(query)):\n",
    "            term=query[j]\n",
    "            doc_list_position.append(di.get(term)[1].get(doc_id))\n",
    "            \n",
    "#         for k in doc_list_position:\n",
    "#             print(k)\n",
    "        \n",
    "#         print('==>',doc_list_position)\n",
    "        for k in range(1,len(doc_list_position)):\n",
    "            doc_list_position[k]=[x-k for x in doc_list_position[k]]\n",
    "#         print('==>',doc_list_position)\n",
    "        \n",
    "        dc_list_set=[]\n",
    "        for k in doc_list_position:\n",
    "            dc_list_set.append(set(k))\n",
    "#         print(dc_list_set)\n",
    "        \n",
    "        if(bool(set.intersection(*dc_list_set))!=0):\n",
    "            searched_docs.append(doc_id)\n",
    "            \n",
    "    return searched_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query:algorithm to\n",
      "algorithm to\n",
      "['algorithm', 'to']\n",
      "['algorithm', 'to']\n",
      "to --> 1764\n",
      "algorithm --> 108\n",
      "Documents with given Phrase: ['comp.graphics/37949', 'comp.graphics/38238', 'comp.graphics/38246', 'comp.graphics/38248', 'comp.graphics/38249', 'comp.graphics/38267', 'comp.graphics/38355', 'comp.graphics/38375', 'comp.graphics/38637', 'comp.graphics/38734', 'comp.graphics/38852', 'comp.graphics/39049', 'comp.graphics/39060']\n",
      "\n",
      " 13\n"
     ]
    }
   ],
   "source": [
    "x=str(input('enter query:'))\n",
    "\n",
    "print(x)\n",
    "x=pre_processing_test(x)\n",
    "print(x)\n",
    "\n",
    "retrieved_docs=search(x)\n",
    "if(len(retrieved_docs)==0):\n",
    "    print('No Documents have this Phrase.')\n",
    "else:\n",
    "    print('Documents with given Phrase:',retrieved_docs)\n",
    "    print('\\n',len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
